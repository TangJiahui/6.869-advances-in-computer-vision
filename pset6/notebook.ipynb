{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"For_students_Pset6_6_819_6_869_Sp22.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"G2haPS8Ig8KP"},"source":["# 6.819 / 6.869 Pset 6 \n","\n","Welcome to Pset 6! The aim of this PSET is to study different issues that appear when deploying models for real applications. These issues lead to a wide range of societal and ethical issues, some of which this PSET will try to briefly illustrate. \n","\n","## Logistics\n","Read all the contents of this cell carefully. \n","\n","**Submissions are due by Thursday 04/07/2022 at 11:59PM**\n","\n","To speed up the execution of the experiments we recommend using GPU acceleration. If you run this in Google Colab, simply do:\n","\n","Runtime > Change Runtime type > Hardware accelerator > GPU.\n","\n","Training a single model with the default parameters for 40 epochs will take less than 15 minutes using GPU in Google Colab (which should be available to you for free) or 3.5 hours using a CPU. You should at least train one model for Q1), but the rest of the PSET can be completed either with your model or a model provided by the instructors.\n","\n","If you get the message: \"Notebook requires high RAM\", press OK and ignore it.\n"]},{"cell_type":"markdown","source":["# **Q0)** Set-up the environment and download resources\n","\n"],"metadata":{"id":"MEcJX3IWXJfT"}},{"cell_type":"markdown","source":["If you want to store data and models in your drive, set the following variable use_gdrive to True, and possibly modify *data_dir* to point to a specific folder. This will allow you to avoid retraining models if your notebook crashes. If set to True, it will ask for permission for the Colab to access your Google Drive."],"metadata":{"id":"32V47hQk1An5"}},{"cell_type":"code","source":["use_gdrive = False"],"metadata":{"id":"0sTRbH5E06L3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from tqdm import tqdm\n","\n","if use_gdrive:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  data_dir = \"/content/drive/MyDrive/notebooks_data/Pset6/data\"\n","else:\n","  data_dir = \"./data\"\n","\n","downloads_dir = data_dir + '/downloads'\n","datasets_dir = data_dir + '/datasets'\n","models_dir = data_dir + '/models'\n","pretrained_models = data_dir + '/pretrained_models'\n","q2c_data_dir = data_dir + '/q2c_data'\n","\n","os.makedirs(downloads_dir, exist_ok=True)\n","os.makedirs(datasets_dir, exist_ok=True)\n","os.makedirs(models_dir, exist_ok=True)\n","os.makedirs(pretrained_models, exist_ok=True)\n","os.makedirs(q2c_data_dir, exist_ok=True)\n","\n","mias_dataset_dir = datasets_dir + '/mias_dataset'\n","external_dataset_dir = datasets_dir + '/external_dataset'\n"],"metadata":{"id":"HiwPBqmGBXY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download MIAS dataset\n","!wget http://6.869.csail.mit.edu/sp22/psets/pset6_serc_data/mias_dataset.zip -O {downloads_dir}/mias_dataset.zip\n","!wget http://6.869.csail.mit.edu/sp22/psets/pset6_serc_data/external_dataset.zip -O {downloads_dir}/external_dataset.zip\n","!wget http://6.869.csail.mit.edu/sp22/psets/pset6_serc_data/q2c_data.zip -O {downloads_dir}/q2c_data.zip\n","\n","!unzip -o {downloads_dir}'/mias_dataset.zip' -d {mias_dataset_dir}\n","!unzip -o {downloads_dir}'/external_dataset.zip' -d {external_dataset_dir}\n","!unzip -o {downloads_dir}'/q2c_data.zip' -d {q2c_data_dir}\n","\n","!rm -rf {downloads_dir}\n","\n","# Download instructors pretrained model\n","!wget http://6.869.csail.mit.edu/sp22/psets/pset6_serc_data/pretreained_models/instructors_model/model.pth -O {pretrained_models}/instructors_model.pth"],"metadata":{"id":"STIAimMmXUSf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6eJKGZ-HulBG"},"source":["# **Q1)** Train an image classifier (2 points)\n","\n","To begin with, you will use the experience you gained in Pset 5 to train a simple image model, that learns to produce a diagnosis in the categories: *normal*, *malignant* and *benign*. Experiment with some of the following hyperparameters / techniques, by modifying the cells that follow:\n","\n","*   Starting with a pretrained model\n","*   Data augmentation\n","*   Weight initialization\n","*   Residual connections (ResNet) and dense connections (DenseNet), compared to networks without them (VGGNet, AlexNet)\n","*   Number of layers, or number of layer features\n","*   Type of optimizer\n","*   Learning rate and/or schedule\n","*   Early stopping\n","\n","\n","For the final set of technique you choose, explain how they affect performance and (optionally), train using different hyperparameters. Your trained network should achieve a validation accuracy of at least 90%. Report the train and test Top-1 accuracy of your final network. \n","\n","Then, save the best performing model for analysis on the following questions (or you can use the instructors provided network), and report your design choices in your final document (such as ResNet-18, data augmentation by rotation, SGD optimizer, learning rate of 0.5, 10% dropout)"]},{"cell_type":"markdown","metadata":{"id":"ZBOaBfJkg8Kb"},"source":["## Section 1: Training a Model\n","\n","This code was adapted from <https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html>\n","\n","You are free to delete this code entirely and start from scratch, or modify it in whatever way you choose to implement your proposed modifications."]},{"cell_type":"markdown","metadata":{"id":"iQslchnXg8Kc"},"source":["### Dependencies"]},{"cell_type":"code","metadata":{"id":"nQ8_lvaFg8Kc"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","# You might not have tqdm, which gives you nice progress bars\n","!pip install tqdm\n","from tqdm.notebook import tqdm\n","import os\n","import copy\n","import pandas as pd\n","import PIL \n","  \n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(\"Using the GPU!\")\n","else:\n","    print(\"WARNING: Could not find GPU! Using CPU only\")\n","    print(\"You may want to try to use the GPU in Google Colab by clicking in:\")\n","    print(\"Runtime > Change Runtime type > Hardware accelerator > GPU.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DC-GQcIjg8Kd"},"source":["### Initialize an Empty Model\n","\n","First, we need to initialize an empty model, that will input an image, and output a classification. Each model is a little different, so we'll make a helper function that takes in an architecture name, and outputs a model. This is only meant as a guideline, and you can try using different models! `torchvision.models` has other common architectures, and variations on these (like ResNet-50 and ResNet-101), so you may want to try those out.\n","\n","We also add a `resume_from` argument to specify model weights to load, In case you save a model and want to use it again."]},{"cell_type":"code","metadata":{"id":"4WB5uFRkg8Kd"},"source":["def initialize_model(model_name, num_classes, resume_from = None, use_pretrained = False):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    # The model (nn.Module) to return\n","    model_ft = None\n","    # The input image is expected to be (input_size, input_size)\n","    input_size = 0\n","    \n","    # By default, all parameters will be trained (useful when you're starting from scratch)\n","    # Within this function you can set .requires_grad = False for various parameters, if you\n","    # don't want to learn them\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"\n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","        \n","    elif model_name == \"resnet50\":\n","        \"\"\" Resnet50\n","        \"\"\"\n","        model_ft = models.resnet50(pretrained=use_pretrained)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n","        input_size = 224\n","\n","    else:\n","        raise Exception(\"Invalid model name!\")\n","    \n","    if resume_from is not None:\n","        print(\"Loading weights from %s\" % resume_from)\n","        model_ft.load_state_dict(torch.load(resume_from))\n","    \n","    return model_ft, input_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"inFD-G_ug8Kf"},"source":["### Data Loading\n","\n","With the input size from the model, we can now load the dataset"]},{"cell_type":"code","metadata":{"id":"HKhoANULg8Kf"},"source":["from torchvision.transforms.functional import to_grayscale\n","\n","def get_image_transforms():\n","    # How to transform the image when you are loading them.\n","    # you'll likely want to mess with the transforms on the training set.\n","    \n","    # we convert the image to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants\n","    # are derived from aggregating lots of data and happen to produce better results.\n","    transform = transforms.Compose([\n","            transforms.Grayscale(num_output_channels=3),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","    return transform\n","\n","def get_dataloaders(dataset_dir, input_size, batch_size, shuffle = True, transform=get_image_transforms()):\n","    data_transforms = {\n","        'train': transform,\n","        'val': transform,\n","        'test': transform\n","    }\n","    # Create training, validation and test datasets\n","    image_datasets = {x: datasets.ImageFolder(os.path.join(dataset_dir, x), data_transforms[x]) for x in data_transforms.keys()}\n","    # Create training, validation and test dataloaders\n","    # Never shuffle the test set\n","    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=False if x != 'train' else shuffle, num_workers=4) for x in data_transforms.keys()}\n","    return dataloaders_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d3YhVPgNg8Kg"},"source":["### Training\n","Next, let's make a helper function that trains the given model"]},{"cell_type":"code","metadata":{"id":"MSgcSULBg8Kh"},"source":["def train_model(model, dataloaders, criterion, optimizer, save_dir = None, save_all_epochs=False, num_epochs=25):\n","    '''\n","    model: The NN to train\n","    dataloaders: A dictionary containing at least the keys \n","                 'train','val' that maps to Pytorch data loaders for the dataset\n","    criterion: The Loss function\n","    optimizer: The algorithm to update weights \n","               (Variations on gradient descent)\n","    num_epochs: How many epochs to train for\n","    save_dir: Where to save the best model weights that are found, \n","              as they are found. Will save to save_dir/weights_best.pt\n","              Using None will not write anything to disk\n","    save_all_epochs: Whether to save weights for ALL epochs, not just the best\n","                     validation error epoch. Will save to save_dir/weights_e{#}.pt\n","    '''\n","    since = time.time()\n","\n","    val_acc_history = []\n","    train_acc_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            # TQDM has nice progress bars\n","            for inputs, labels in tqdm(dataloaders[phase]):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","\n","                    # torch.max outputs the maximum value, and its index\n","                    # Since the input is batched, we take the max along axis 1\n","                    # (the meaningful outputs)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backprop + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","            \n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'train':\n","                train_acc_history.append(epoch_acc)\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","            if save_all_epochs:\n","                torch.save(model.state_dict(), os.path.join(save_dir, f'weights_{epoch}.pt'))\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # save and load best model weights\n","    torch.save(best_model_wts, os.path.join(save_dir, 'weights_best_val_acc.pt'))\n","    torch.save(model.state_dict(), os.path.join(save_dir, 'weights_last.pt'.format(epoch)))\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history, train_acc_history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CKECnWtqg8Kj"},"source":["### Optimizer & Loss\n","We need a loss function, and an optimization function to use to try to reduce that loss."]},{"cell_type":"code","metadata":{"id":"rigJ--Oeg8Kl"},"source":["def make_optimizer(model, learning_rate, print_parameters=False):\n","    # Get all the parameters\n","    params_to_update = model.parameters()\n","    if print_parameters:\n","      print(\"Params to learn:\")\n","      for name, param in model.named_parameters():\n","          if param.requires_grad == True:\n","              print(\"\\t\",name)\n","\n"," \n","    optimizer = optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n","    return optimizer\n","\n","def get_loss():\n","    # Create an instance of the loss function\n","    criterion = nn.CrossEntropyLoss()\n","    return criterion"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KiJMbW15g8Km"},"source":["### Parameters\n","\n","Here, we set up some of the various parameters that we can change to run the code. You can add change the values given here, or add new ones! This is just a template.\n","\n","Our data is conveniently set up to follow the expected format of the  `ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>`__\n","dataset class, rather than writing our own custom dataset.\n","\n","The ``model_name`` input is the name of the model you wish to use. We've provided starter code that initializes these models using provided models in TorchVision (a PyTorch library)\n","\n","The code as is supports the following values: [resnet, alexnet, vgg, squeezenet, densenet]\n","\n","The other inputs are as follows: ``num_classes`` is the number of\n","classes in the dataset, 3 here, ``batch_size`` is the batch size used for\n","training and may be adjusted according to the capability of your\n","machine, ``num_epochs`` is the number of training epochs (passes through the dataset) we want to run.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"jeeXTYlKg8Km"},"source":["# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n","# You can add your own, or modify these however you wish!\n","model_name = ###Your own###\n","\n","# Number of classes in the dataset, normal, benign, malignant\n","num_classes = 3\n","\n","# Batch size for training (change depending on how much memory you have)\n","batch_size = 32\n","\n","# Shuffle the input data?\n","shuffle_datasets = True\n","\n","# Number of epochs to train for \n","num_epochs = ###Your own###\n","\n","# Learning rate\n","learning_rate = ###Your own###\n","\n","### IO\n","# Path to a model file to use to start weights at\n","resume_from = None\n","\n","# Whether to use a pretrained model, trained for classification in Imagenet-1k \n","pretrained = False\n","\n","# Save all epochs so that you can select the model from a particular epoch\n","save_all_epochs = False\n","\n","# Whether to use early stopping (load the model with best accuracy), or not\n","early_stopping = True\n","\n","# Directory to save weights to\n","save_dir = models_dir + '/trained_model_1'\n","os.makedirs(save_dir, exist_ok=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sfvtMRQ6g8Km"},"source":["### Tying it all together - Training"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"xPXX3aEdg8Kn"},"source":["# Initialize the model for this run\n","# train model_1\n","model_1, input_size = initialize_model(model_name = model_name, num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n","dataloaders = get_dataloaders(mias_dataset_dir, input_size, batch_size, shuffle_datasets)\n","criterion = get_loss()\n","\n","# Move the model to the gpu if needed\n","model_1 = model_1.to(device)\n","\n","optimizer_1 = make_optimizer(model_1, learning_rate)\n","\n","# Train the model!\n","trained_model_1, validation_history_1, train_history_1 = train_model(model=model_1, \n","                                                                     dataloaders=dataloaders, \n","                                                                     criterion=criterion, \n","                                                                     optimizer=optimizer_1,\n","                                                                     save_dir=save_dir, \n","                                                                     save_all_epochs=save_all_epochs, \n","                                                                     num_epochs=num_epochs)\n","del model_1, optimizer_1, trained_model_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Select the model you will use for the rest of the PSET"],"metadata":{"id":"xxxaZu-5SWtO"}},{"cell_type":"code","source":["# Load your final model, that we will use for the rest of the PSET.\n","if early_stopping:\n","  weights_file = save_dir + '/weights_best_val_acc.pt'\n","else:\n","  weights_file = save_dir + '/weights_last.pt'\n","model_yours, _ = initialize_model(model_name = model_name, num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n","\n","# Move the model to the gpu if needed\n","model_yours = model_yours.to(device)\n","\n","# Load weights for model_yours\n","model_yours.load_state_dict(torch.load(weights_file))\n","\n","# Load instructors model:\n","\n","model_inst = torch.load(pretrained_models + '/instructors_model.pth')\n","model_inst = model_inst.to(device)\n","\n","\n","# set models to eval mode\n","model_yours = model_yours.eval()\n","model_inst = model_inst.eval()\n","\n"],"metadata":{"id":"WBt3EbutF7YR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89kemy1Pg8Kn"},"source":["\n","## Section 2: Inference using a model\n","\n","Now that we've trained a model, we would like to use it for inference on the test data. We will use a function that can compute top-k performance (i.e. whether the correct prediction is in the top-k predicted classes), but for this pset, we will keep k = 1, as there are only 3 classes.\n"]},{"cell_type":"code","metadata":{"id":"Nlkyj8qhg8Kn"},"source":["def evaluate(model, dataloader, criterion, is_labelled = False, generate_labels = True, k = 5):\n","    # If is_labelled, we want to compute loss, top-1 accuracy and top-5 accuracy\n","    # If generate_labels, we want to output the actual labels\n","    # Set the model to evaluate mode\n","    model.eval()\n","    running_loss = 0\n","    running_top1_correct = 0\n","    running_top5_correct = 0\n","    predicted_labels = []\n","    gt_labels = []\n","\n","    # Iterate over data.\n","    # TQDM has nice progress bars\n","    for inputs, labels in tqdm(dataloader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        tiled_labels = torch.stack([labels.data for i in range(k)], dim=1) \n","        # Makes this to calculate \"top 5 prediction is correct\"\n","        # [[label1 label1 label1 label1 label1], [label2 label2 label2 label label2]]\n","\n","        # forward\n","        # track history if only in train\n","        with torch.set_grad_enabled(False):\n","            # Get model outputs and calculate loss\n","            outputs = model(inputs)\n","            if is_labelled:\n","                loss = criterion(outputs, labels)\n","\n","            # torch.topk outputs the maximum values, and their indices\n","            # Since the input is batched, we take the max along axis 1\n","            # (the meaningful outputs)\n","            _, preds = torch.topk(outputs, k=k, dim=1)\n","            if generate_labels:\n","                # We want to store these results\n","                nparr = preds.cpu().detach().numpy()\n","                predicted_labels.extend([list(nparr[i]) for i in range(len(nparr))])\n","                gt_labels.extend(np.array(labels.cpu()))\n","\n","        if is_labelled:\n","            # statistics\n","            running_loss += loss.item() * inputs.size(0)\n","            # Check only the first prediction\n","            running_top1_correct += torch.sum(preds[:, 0] == labels.data)\n","            # Check all 5 predictions\n","            running_top5_correct += torch.sum(preds == tiled_labels)\n","        else:\n","            pass\n","\n","    # Only compute loss & accuracy if we have the labels\n","    if is_labelled:\n","        epoch_loss = float(running_loss / len(dataloader.dataset))\n","        epoch_top1_acc = float(running_top1_correct.double() / len(dataloader.dataset))\n","        epoch_top5_acc = float(running_top5_correct.double() / len(dataloader.dataset))\n","    else:\n","        epoch_loss = None\n","        epoch_top1_acc = None\n","        epoch_top5_acc = None\n","    \n","    # Return everything\n","    return epoch_loss, epoch_top1_acc, gt_labels, predicted_labels  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hx-PjTCxg8Ko"},"source":["# Get data on the validation set\n","# Setting this to false will be a little bit faster\n","generate_validation_labels = True\n","val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_yours, dataloaders['val'], criterion, is_labelled = True, generate_labels = generate_validation_labels, k = 1)\n","# Get predictions for the test set\n","test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_yours, dataloaders['test'], criterion, is_labelled = True, generate_labels = generate_validation_labels, k = 1)\n","\n","val_loss_inst, val_top1_inst, _, val_labels_inst = evaluate(model_inst, dataloaders['val'], criterion, is_labelled = True, generate_labels = generate_validation_labels, k = 1)\n","# Get predictions for the test set\n","test_loss_inst, test_top1_inst, _, test_labels_inst = evaluate(model_inst, dataloaders['test'], criterion, is_labelled = True, generate_labels = generate_validation_labels, k = 1)\n","\n","print(\"Your Trained model: \")\n","print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n","print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n","\n","print(\"Instructors model: \")\n","print(\"Val Top-1 Accuracy: {}\".format(val_top1_inst))\n","print(\"Test Top-1 Accuracy: {}\".format(test_top1_inst))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Select either your model or the instructors model for the next questions. If your model does not achieve high accuracy (>=0.93 on the test set), you should use the instructors model (model_inst)."],"metadata":{"id":"mF_Yp64l93xM"}},{"cell_type":"code","source":["model = model_inst\n","# model = model_yours"],"metadata":{"id":"ZwlV_tsE99oV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q2)** Accuracy isn't everything (4 points)\n","\n","In the previous question, you may have achieved high accuracy, but that doesn't necessarily mean that the system is uesful for diagnosis in practice. Suppose that you model just learns nothing and outputs *normal* for every image. If 90%  of the patient images in your dataset are *normal*, the accuracy is as high as 90%! \n","\n","In this question, we will introduce some other metrics commonly used in biomedical imaging tasks. First, we define a function to get detail evaluation results on the test set:"],"metadata":{"id":"UJ9rANYC4S2r"}},{"cell_type":"code","source":["# Warpper to easily evaulate a model given a model and the set of dataloaders\n","def get_eval_results(model, dataloaders):\n","    model.eval()\n","    true_label_list = []\n","    outputs_list = []\n","    predicted_label_list = []\n","    original_image_list = []\n","\n","    # TQDM has nice progress bars\n","    for inputs, labels in tqdm(dataloaders['test']):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        with torch.set_grad_enabled(False):\n","            # Get model outputs and calculate loss\n","            outputs = model(inputs)\n","            true_label_list.append(labels)\n","            original_image_list.append(inputs)\n","            outputs_list.append(outputs)\n","            _, preds = torch.topk(outputs, k=1, dim=1)\n","            predicted_label_list.append(preds)\n","    return torch.concat(true_label_list).unsqueeze(-1).cpu().numpy(), \\\n","           torch.concat(predicted_label_list).cpu().numpy(), \\\n","           torch.softmax(torch.concat(outputs_list), dim=1).cpu().numpy(), \\\n","           torch.concat(original_image_list).cpu().numpy()\n","\n","## Please make sure you understand what outputs means here\n","y_label, y_pred, outputs, inputs =  get_eval_results(model, dataloaders)"],"metadata":{"id":"8igSXYxd5e6R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","###**Q2 a)** Confusion matrix (1 point)\n","\n","The first simple method that we will use to delve deeper into the results is the confusion matrix.\n"],"metadata":{"id":"D0-_Xpi-Oqvy"}},{"cell_type":"code","source":["### Complete your code here to plot your confusion matrix \n","### please plot it without using metrics packages here for better understanding \n","\n","def plot_confusion_matrix(y_label, y_pred, title='Confusion matrix'):\n","    ### TODO 1\n","  \n","plot_confusion_matrix(y_label, y_pred)"],"metadata":{"id":"iYEcYY4O2EH6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Q2 b)** AUPRC (1 point)\n","***Important: Our dataset has 3 classes. In order to use some definitions in binary classification tasks, you need to create binary labels from the given class labels. Imagine a scenario where you really care about malignant cases. Please consider malignant class (label 1) to be the positive class. And both benign (label 0) and normal (label 2) classes are negative***\n","\n","\n"],"metadata":{"id":"wOU8QEEVPBe7"}},{"cell_type":"code","source":["### write your code here to plot your AUPRC curve for the model \n","### please plot it without using metrics packages here for better understanding \n","\n","### use the returned values from the get_aval_results() above \n","### TODO 2\n","\n"],"metadata":{"id":"juRxsLYZJKQA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Q2 c)** Fairness Metrics (2 points)\n","***Important: please only use malignant class (class label 1) as positive, and both benign and normal to be negative.***"],"metadata":{"id":"V6xnnllWAVOW"}},{"cell_type":"code","source":["# fairness_vgg_1.csv fairness_vgg_2.csv which has age distribution for the malignant class of the datasets that these models were trained on\n","# q2c_data_dir/test: a folder with test images\n","# test_age.csv: a csv with test image names and its corresponding age\n","# fairness_vgg_1.pt and fairness_vgg_2.pt are the pretrained weights  \n","\n","# create 2 empty vgg models and load pre-trained weights \n","fairness_model_name = \"vgg\"\n","\n","fairness_model_1, input_size = initialize_model(model_name = fairness_model_name, num_classes = num_classes, resume_from = None)\n","fairness_model_2, input_size = initialize_model(model_name = fairness_model_name, num_classes = num_classes, resume_from = None)\n","fairness_model_1 = fairness_model_1.to(device)\n","fairness_model_2 = fairness_model_2.to(device)\n","fairness_model_1.load_state_dict(torch.load(q2c_data_dir +'/fairness_vgg_1.pt'))\n","fairness_model_2.load_state_dict(torch.load(q2c_data_dir +'/fairness_vgg_2.pt'))\n","\n","\n","# prepare fairness_dataloader using ImageFolder again and please do not change shuffle=False\n","fairness_testset= datasets.ImageFolder(os.path.join(q2c_data_dir, 'test'), get_image_transforms())\n","dataloader_fair = torch.utils.data.DataLoader(fairness_testset, batch_size=32, shuffle=False)\n","fairness_dataloader = {'test': dataloader_fair}\n","\n","\n","fairness_test_age = pd.read_csv(q2c_data_dir+'/test_age.csv', header=None,  names=['img_name', 'age'])\n","age_list= []\n","# get an age_list which should corresponds to the evaluation results \n","# This is what ImageFolder source uses. Use this to make sure age_list is ordered the same as the fairness_dataloader\n","for root, _, fnames in sorted(os.walk(os.path.join(q2c_data_dir, 'test'), followlinks=True)):\n","    for fname in sorted(fnames):\n","        age_list.append(fairness_test_age[fairness_test_age.loc[:, 'img_name'] == fname].loc[:, 'age'].values[0])\n","\n","# Finally let's run get_eval_results on this!\n","y_label, y_pred_f1, outputs_f1, inputs =  get_eval_results(fairness_model_1, fairness_dataloader)\n","y_label, y_pred_f2, outputs_f2, inputs =  get_eval_results(fairness_model_2, fairness_dataloader)"],"metadata":{"id":"p81dNMlVda45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TODO now it is your turn to evaluate the model based on our instructions in the PDF\n","### use y_label, y_pred_f1, y_pred_f2 and age_list\n","### focus on malignant class which has class label 1 \n","\n","### TODO 3\n","\n"],"metadata":{"id":"U3ApPpC6nWC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q3)** Where is the model looking at? (3 points)\n","\n","A common technique used to delve deeper into trained models are visualizations. As studied in the previous PSET, filter visualizations are seometimes useful to understand how the network behaves, but it is hard for non-expert users to interpret them.\n","\n","## **Q3 a)** Grad-CAM\n","\n","Another technique studied is CAM visualizations, which provide a simple way to interpret what regions of the image contribute to the prediction for each output target. This is a useful tool to check whether the model is picking up spuroious correlations or the predictions focus on regions of the image that are consistent with expert knowledge.\n","\n","In the following code snippet, we use the following pytorch package that implements different variatinos of CAM:\n","https://github.com/jacobgil/pytorch-grad-cam\n","\n","Run Grad-CAM (or another of the CAM visualization) on several images and classes for the different test sets. Provide some examples in your notebook and explain the differences you see between correct and incorrect classified samples."],"metadata":{"id":"Tn8DvSZBPfBu"}},{"cell_type":"code","source":["!pip install grad-cam\n","\n","import random\n","from collections import defaultdict\n","\n","from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n"],"metadata":{"id":"qJ56Koj9D_-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_cams(target_dataloaders, cam_method=GradCAM, class_id='gt'):\n","  assert class_id in ['gt', 'pred'] or type(class_id) is int and class_id < 3\n","  for split in ['train', 'val', 'test']:\n","    target_layers = [model.layer4[-1]]\n","    fig, axs = plt.subplots(2, 6, figsize=(14, 7))\n","    fig.suptitle(\"Examples split {}, class {}\".format(split, class_id), fontsize=16)\n","\n","    split_dataset = target_dataloaders[split].dataset\n","    indices_per_class = defaultdict(list)\n","    for i, (_, c) in enumerate(split_dataset.imgs):\n","      indices_per_class[c].append(i)\n","\n","    random.seed(1337)  \n","    indices = []\n","    for c in range(3):\n","      indices.extend(random.sample(indices_per_class[c], 4))\n","\n","    for dataset_i, ax in zip(indices, axs.flatten()):\n","      input_tensor, class_idx = split_dataset[dataset_i]\n","      input_tensor = input_tensor[None,...].cuda()\n","\n","      # Construct the CAM object once, and then re-use it on many images:\n","      cam = cam_method(model=model, target_layers=target_layers, use_cuda=not device == 'cpu')\n","      pred_class_idx = model(input_tensor).argmax()\n","\n","      # We have to specify the target we want to generate\n","      # the Class Activation Maps for\n","      if class_id is int:\n","        target_class_id = class_id\n","      else:\n","        target_class_id = pred_class_idx if class_id == 'pred' else class_idx        \n","      targets = [ClassifierOutputTarget(target_class_id)]\n","\n","      # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n","      grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n","\n","      # In this example grayscale_cam has only one image in the batch:\n","      grayscale_cam = grayscale_cam[0, :]\n","      rgb_image = np.array(input_tensor[0].cpu())\n","      rgb_image = (rgb_image - rgb_image.min())/ (rgb_image.max() - rgb_image.min())\n","      visualization = show_cam_on_image(rgb_image.transpose((1,2,0)), grayscale_cam, use_rgb=True)\n","      \n","      ax.set_title('True : %s\\n Predicted: %s' %(split_dataset.classes[class_idx], split_dataset.classes[pred_class_idx]))\n","      ax.imshow(visualization)\n"],"metadata":{"id":"buxAeJL-DmAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Substitute GradCAM for one of the following, to test other methods:\n","# GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n","# Also substitute class_id for pred, or a class id in [0,1,2] to check different output.\n","plot_cams(dataloaders, cam_method=GradCAM, class_id='gt')"],"metadata":{"id":"z4KSEF-QPrHG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Q4) Evaluate on external dataset (2 points)\n"],"metadata":{"id":"CHFowltG4oqV"}},{"cell_type":"markdown","source":["In this question we will study a problem that occurs frequently in practice when deploying systems. Oftentimes, the data distribution at test time is different than the one used in training.\n","\n","To begin with, compare the performance with the original model against an external dataset of the same type, provided in data/external_dataset."],"metadata":{"id":"iGdxXJq1BtmT"}},{"cell_type":"code","source":["dataloaders_external = get_dataloaders(external_dataset_dir, input_size, batch_size, shuffle_datasets)\n","\n","test_loss, test_top1, gt_labels, pred_labels = evaluate(model, dataloaders['test'], criterion, is_labelled = True, generate_labels = True, k = 1)\n","test_loss_ext, test_top1_ext, gt_labels_ext, pred_labels_ext = evaluate(model, dataloaders_external['test'], criterion, is_labelled = True, generate_labels = True, k = 1)\n","\n","print(\"Top-1 Acc: {}\".format(test_top1))\n","print(\"External Top-1 Acc: {}\".format(test_top1_ext))\n","\n","plot_confusion_matrix(gt_labels_ext, pred_labels_ext, title='CM External dataset')\n"],"metadata":{"id":"rUdFNrjOERBO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, performance is worse on the external dataset. Compare  samples of the two datsets visually, and report the differences that you appreciate.\n","\n"],"metadata":{"id":"Q_lGf51MB_cD"}},{"cell_type":"code","source":["def plot_samples(dataloaders_external, title=''):\n","  fig, axs = plt.subplots(2, 6, figsize=(14, 7))\n","  fig.suptitle(title, fontsize=16)\n","\n","  dataset = dataloaders_external['test'].dataset\n","  indices_per_class = defaultdict(list)\n","  for i, (_, c) in enumerate(dataset.imgs):\n","    indices_per_class[c].append(i)\n","\n","  random.seed(1337)  \n","  indices = []\n","  for c in range(3):\n","    indices.extend(random.sample(indices_per_class[c], 4))\n","\n","  # remove normalize transforms for plotting\n","    inv_normalize = transforms.Normalize(\n","        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n","        std=[1/0.229, 1/0.224, 1/0.255]\n","    )\n","\n","  for dataset_i, ax in zip(indices, axs.flatten()):\n","    img, class_idx = dataset[dataset_i]\n","    img = inv_normalize(img)\n","    ax.set_title('True : %s' %(dataset.classes[class_idx]))\n","    if len(img.shape) == 2:\n","      img = np.tile(img[:,:,None], (1,1,3))\n","    img = np.array((img*255).clip(0, 255), dtype='uint8')\n","    ax.imshow(img.transpose((1,2,0)))\n","\n","# Plot some samples of each of the datasets:\n","plot_samples(dataloaders, 'Original dataset samples')\n","plot_samples(dataloaders_external, 'External dataset samples')"],"metadata":{"id":"2NUs5KsDZe_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(6.869 only) Implement a set of transformations that can be applied to the original dataset at train time that would mitigate this issue. Plot the original dataset with the proposed transformations, which should produce qualitatively similar samples. You don't need to retrain the model, though you are free to do so to further check that the transformations are correct (performance on the external dataset should improve using the original data with the extra transformations). Use some of the transformations in: \n","\n","https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor\n","\n"],"metadata":{"id":"JJeW3oB4CKEL"}},{"cell_type":"code","source":["from torchvision.transforms.functional import to_grayscale\n","# TODO:\n","external_transforms = transforms.Compose([ # add the transforms here, and screenshot the transforms on the report\n","                                          \n","\n","])\n","# END TODO\n","transform = transforms.Compose([external_transforms,\n","                                get_image_transforms()])\n","\n","modified_original_dataloaders = get_dataloaders(mias_dataset_dir, input_size, batch_size, shuffle_datasets, transform)\n","plot_samples(modified_original_dataloaders, 'Original dataset samples')\n","plot_samples(dataloaders_external, 'External dataset samples')"],"metadata":{"id":"xxM7Pse_ZlR0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Q5)** Class imbalance (2 points)\n","\n","An issue typically present in medical datasets is class imbalance: patients that do not present the disease are more frequent than patients that do have the disease. One way to make the trained model perform differently is by simulating a dataset that contains a different number of samples of each category. This can be achieved by at least two methods: reweighting of the loss and changing the sampling procedure."],"metadata":{"id":"kEqsskPZd3ZK"}},{"cell_type":"code","source":["from collections import defaultdict \n","# First we will compute statistics of the dataset:\n","dataloaders = get_dataloaders(mias_dataset_dir, input_size, batch_size, shuffle_datasets)\n","train_dataset = dataloaders['train'].dataset\n","\n","samples_per_class = [0 for _ in range(3)]\n","for _, class_idx in train_dataset:\n","  samples_per_class[class_idx] += 1\n","\n","print(\"Samples per class:\")\n","for c_idx in range(3):\n","  print(\"Class {} ({}): {} samples\".format(c_idx, train_dataset.classes[c_idx], samples_per_class[c_idx]))\n","\n"],"metadata":{"id":"dCOPnpoaXVIM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Q5 a)** Loss reweigthing\n","\n","Complete the following lines below to reweight the loss so that the three classes are balanced. Then, excecute the training loop."],"metadata":{"id":"1bCbjHehhn-s"}},{"cell_type":"code","source":["total_samples = sum(samples_per_class)\n","\n","# TODO: add the weights per class so that the loss is balanced. \n","# The sum of weights over all samples should sum up to total_samples (i.e. the same as if the weights where 1) \n","weights_per_class = ...\n","# ENDTODO\n","\n","reweighted_criterion = nn.CrossEntropyLoss(weight=weights_per_class)      "],"metadata":{"id":"S9ed5kL35mmt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Q5 b)** Dataset rebalancing\n","Complete the following lines to modify the data loader so that samples for the three classes are balanced. For simplicity, you are allowed to select an element at random, and ignore the *item* argument of the dataset.__getitem__ function.\n","Then, excecute the training loop with the modified dataset/dataloader."],"metadata":{"id":"JM0CtdkjjF28"}},{"cell_type":"code","source":["import random\n","\n","from collections import defaultdict\n","class RebalancedDataset(datasets.ImageFolder):\n","  def __init__(self, class_probabilities, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.class_probabilities = class_probabilities\n","    self.items_per_class = defaultdict(list)\n","    \n","    for item_i, (_, c) in enumerate(self.imgs):\n","      self.items_per_class[c].append(item_i)\n","\n","  def __getitem__(self, i_ignored):\n","    # ignores argument, and returns an element at random with reweighted class\n","    if i_ignored >= len(self):\n","      raise IndexError('Index is too large') # Exception required so that the iterator ends after len(self) samples\n","    # TODO: select a class at random following self.class_probabilities, and a random element for the class\n","    class_i = ...\n","    i = ...\n","    # ENDTODO\n","    return super().__getitem__(i)\n"],"metadata":{"id":"t8JRZyR9mQIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replicates the original get_dataloaders, but in this case we substitute the training dataset for the RebalancedDataset\n","def get_dataloaders_rebalanced(class_probabilities, dataset_dir, input_size, batch_size, shuffle = True):\n","    data_transforms = {\n","        'train': get_image_transforms(),\n","        'val': get_image_transforms(),\n","        'test': get_image_transforms()\n","    }\n","\n","    # Create training and validation datasets\n","    image_datasets = {'train': RebalancedDataset(class_probabilities, os.path.join(dataset_dir, 'train'), data_transforms['train']),\n","                      'val': datasets.ImageFolder(os.path.join(dataset_dir, 'val'), data_transforms['val']),\n","                      'test': datasets.ImageFolder(os.path.join(dataset_dir, 'test'), data_transforms['test'])}\n","    # Create training and validation dataloaders\n","    # Never shuffle the test set\n","    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=False if x != 'train' else shuffle, num_workers=4) for x in data_transforms.keys()}\n","    return dataloaders_dict"],"metadata":{"id":"-bTmJQinfn8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_probabilities = [1/3 for _ in range(3)]\n","dataloaders_rebalanced = get_dataloaders_rebalanced(class_probabilities, mias_dataset_dir, input_size, batch_size, shuffle_datasets)\n","\n","train_dataset_rebalanced = dataloaders_rebalanced['train'].dataset\n"],"metadata":{"id":"Db-nFvcZiFtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict \n","\n","samples_per_class_rebalanced = [0 for _ in range(3)]\n","for _, class_idx in tqdm(train_dataset_rebalanced):\n","  samples_per_class_rebalanced[class_idx] += 1\n","\n","print(\"Rebalanced samples per class:\")\n","for c_idx in range(3):\n","  print(\"Class {} ({}): {} samples\".format(c_idx, train_dataset_rebalanced.classes[c_idx], samples_per_class_rebalanced[c_idx]))\n","\n"],"metadata":{"id":"r54M9pTqi5nr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(6.869 only) Finally, train the models with the modified criterion and rebalanced dataloader"],"metadata":{"id":"-AJ2SIQAJq80"}},{"cell_type":"code","source":["model_reweighted, input_size = initialize_model(model_name = model_name, num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n","\n","model_reweighted = model_reweighted.to(device)\n","optimizer_reweighted = make_optimizer(model_reweighted, learning_rate)\n","trained_reweighted, validation_history_reweighted, train_history_reweighted = train_model(model=model_reweighted, \n","                                                                                          dataloaders=dataloaders, \n","                                                                                          criterion=reweighted_criterion, \n","                                                                                          optimizer=optimizer_reweighted,\n","                                                                                          save_dir=save_dir, \n","                                                                                          save_all_epochs=save_all_epochs, \n","                                                                                          num_epochs=num_epochs)"],"metadata":{"id":"Sk1IlkDGJqNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_rebalanced, input_size = initialize_model(model_name = model_name, num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n","\n","model_rebalanced = model_rebalanced.to(device)\n","optimizer_rebalanced = make_optimizer(model_rebalanced, learning_rate)\n","trained_rebalanced, validation_history_rebalanced, train_history_rebalanced = train_model(model=model_rebalanced, \n","                                                                                          dataloaders=dataloaders_rebalanced, \n","                                                                                          criterion=criterion, \n","                                                                                          optimizer=optimizer_rebalanced,\n","                                                                                          save_dir=save_dir, \n","                                                                                          save_all_epochs=save_all_epochs, \n","                                                                                          num_epochs=num_epochs)"],"metadata":{"id":"Sn6jor9biDer"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, plot the confussion matrix for the three models, and explain empirical (if there are) and theoretical differences between the two techniques. \n","Hint: think about the extreme case where the number of samples of the less predominant class is low."],"metadata":{"id":"JOg3BUFtxpXW"}},{"cell_type":"code","source":["y_label_reweighted, y_pred_reweighted, _, _ =  get_eval_results(model, dataloaders)\n","plot_confusion_matrix(y_label_reweighted, y_pred_reweighted, title='CM original')\n","\n","y_label_reweighted, y_pred_reweighted, _, _ =  get_eval_results(model_reweighted, dataloaders)\n","plot_confusion_matrix(y_label_reweighted, y_pred_reweighted, title='CM reweighted')\n","\n","y_label_rebalanced, y_pred_rebalanced, _, _ =  get_eval_results(model_rebalanced, dataloaders)\n","plot_confusion_matrix(y_label_rebalanced, y_pred_rebalanced, title='CM rebalanced')"],"metadata":{"id":"mcDKG0EaFNiA"},"execution_count":null,"outputs":[]}]}